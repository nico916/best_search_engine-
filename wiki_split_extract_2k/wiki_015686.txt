Optimisation linéaire
En optimisation mathématique , un problème d' optimisation linéaire demande de minimiser une fonction linéaire sur un polyèdre convexe .
La fonction que l' on minimise ainsi que les contraintes sont décrites par des fonctions linéaires , d' où le nom donné à ces problèmes .
L' optimisation linéaire ( OL ) est la discipline qui étudie ces problèmes .
Elle est également désignée par le nom de programmation linéaire , terme introduit par George Dantzig vers 1947 , mais cette appellation tend à être abandonnée à cause de la confusion possible avec la notion de programmation informatique .
Par exemple , le problème à deux variables x = ( x 1 , x 2 ) ∈ R 2 { \ displaystyle x = ( x_ { 1 } , x_ { 2 } ) \ in \ mathbb { R } ^ { 2 } } suivant { inf x x 1 + x 2 x 1 + 2 x 2 ⩾ 2 , x 1 ⩾ 0 , x 2 ⩾ 0 , { \ displaystyle \ left \ { { \ begin { array } { l } { \ inf } _ { x } \ ; x_ { 1 } + x_ { 2 } \ \ x_ { 1 } + 2x_ { 2 } \ geqslant 2 , \ quad x_ { 1 } \ geqslant 0 , \ quad x_ { 2 } \ geqslant 0 , \ end { array } } \ right . } qui consiste à minimiser la fonction linéaire x = ( x 1 , x 2 ) ∈ R 2 ↦ x 1 + x 2 ∈ R { \ displaystyle x = ( x_ { 1 } , x_ { 2 } ) \ in \ mathbb { R } ^ { 2 } \ mapsto x_ { 1 } + x_ { 2 } \ in \ mathbb { R } } sous la contrainte d' inégalité affine x1 + 2x2 ≥ 2 et les contraintes de positivité des xi est un problème d' optimisation linéaire .
Sa solution est ( x1 , x2 ) = ( 0,1 ) .
Dès que le nombre de variables et de contraintes augmente , le problème ne peut plus se résoudre par tâtonnement .
Plus généralement , un problème d' OL s' écrira donc en notation matricielle de la manière suivante { inf x c ⊤ x A x ⩽ b , { \ displaystyle \ left \ { { \ begin { array } { l } { \ inf } _ { x } \ ; c ^ { \ top } x \ \ Ax \ leqslant b , \ end { array } } \ right . } où x ∈ R n { \ displaystyle x \ in \ mathbb { R } ^ { n } } est l' inconnue , le vecteur des variables réelles x1 ,..., xn à optimiser , et les données sont des vecteurs c ∈ R n { \ displaystyle c \ in \ mathbb { R } ^ { n } } et b ∈ R m { \ displaystyle b \ in \ mathbb { R } ^ { m } } et une matrice A ∈ R m × n { \ displaystyle A \ in \ mathbb { R } ^ { m \ times n } } .
L' inégalité vectorielle Ax ≤ b doit être entendue composante par composante : pour tout indice i , on doit avoir ( Ax – b ) i ≤ 0 .
